{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b0f31e-ba93-48e5-9bde-f227298ec4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os, random\n",
    "\n",
    "random.seed(42069666)\n",
    "\n",
    "class WikiDataset:\n",
    "    def __init__(self, setting):\n",
    "        self.FOLDERS = ['History', 'Geography', 'Arts', 'Philosophy_and_religion','Everyday_life',\\\n",
    "                  'Society_and_social_sciences','Biological_and_health_sciences', 'Physical_sciences',\\\n",
    "              'Technology', 'Mathematics']\n",
    "        self.MAP = {}\n",
    "        self.CORPUS_FILE = \"corpus.txt\"\n",
    "        self.SPLIT = np.array([0, 0.7,0.85,1])\n",
    "        self.DIM = 0\n",
    "        self.fileList = []\n",
    "        self.setting = setting # one of 0,1,2 for train, test val\n",
    "        for i in range(len(self.FOLDERS)):\n",
    "            self.MAP[self.FOLDERS[i]] = i\n",
    "        \n",
    "        for folder in self.FOLDERS:\n",
    "            for file in os.listdir(folder):\n",
    "                self.fileList.append((self.MAP[folder], os.path.join(folder, file)))\n",
    "        random.shuffle(self.fileList)\n",
    "        self.SPLIT = [int(c) for c in self.SPLIT * len(self.fileList)]\n",
    "        \n",
    "        k = open(self.CORPUS_FILE,\"r\")\n",
    "        while True:\n",
    "            line = k.readline()\n",
    "            if len(line.split()) == 2:\n",
    "                self.DIM += 1\n",
    "            else:\n",
    "                break\n",
    "    def __len__(self):\n",
    "        return self.SPLIT[self.setting+1] - self.SPLIT[self.setting]\n",
    "\n",
    "    def bag_transform(self, bag):\n",
    "        return bag/sum(bag)\n",
    "    def __getitem__(self, idx):\n",
    "        # we use log bag of words due to zipfian nature\n",
    "        idx += self.SPLIT[self.setting]\n",
    "        k = open(self.fileList[idx][1], \"r\")\n",
    "        bag = np.zeros(self.DIM)\n",
    "\n",
    "        while True:\n",
    "            l = k.readline().split()\n",
    "            if len(l) <= 1:\n",
    "                break\n",
    "            bag[int(l[0])] = float(l[1])\n",
    "        return self.bag_transform(bag), self.fileList[idx][0]\n",
    "\n",
    "TrainSet = WikiDataset(0)\n",
    "ValSet = WikiDataset(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5ec278-a066-4597-9814-d12a9289967e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5567/5567 [02:43<00:00, 34.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1193/1193 [00:31<00:00, 37.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7820620284995808"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = []\n",
    "ytrain = []\n",
    "Xval = []\n",
    "yval = []\n",
    "for i in tqdm(range(TrainSet.__len__())):\n",
    "    #if i % 50 == 0:\n",
    "    #    print(f\"Loaded {i} Train Examples\")\n",
    "    a,b = TrainSet.__getitem__(i)\n",
    "    Xtrain.append(a)\n",
    "    ytrain.append(b)\n",
    "\n",
    "for i in tqdm(range(ValSet.__len__())):\n",
    "    #if i % 50 == 0:\n",
    "    #    print(f\"Loaded {i} Test Examples\")\n",
    "    a,b = ValSet.__getitem__(i)\n",
    "    Xval.append(a)\n",
    "    yval.append(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0a6bde-026f-47f9-9f9c-04c73501603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                              | 86/3342 [00:05<02:32, 21.39it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      " 95%|██████████████████████████████████████████████████████████████████████████▏   | 3176/3342 [01:18<00:03, 55.01it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3342/3342 [01:21<00:00, 40.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/0 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from benchmark_base import CategoriserBase\n",
    "\n",
    "class SVM_tester(CategoriserBase):\n",
    "    def __init__(self):\n",
    "        super(SVM_tester, self).__init__()\n",
    "    \n",
    "    def precomp(self):\n",
    "        # override\n",
    "        self.model = svm.LinearSVC(C=25)\n",
    "        self.model.fit(Xtrain, ytrain)\n",
    "        print('Finished training')\n",
    "    def predict(self, bag):\n",
    "        return self.model.decision_function(bag)\n",
    "    \n",
    "SVM_instance = SVM_tester()\n",
    "SVM_instance.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce6f9dc9-c42b-42c6-990c-7c9e0769b6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.03273518,  0.48975202, -1.22076547, -1.15381653, -0.94391725,\n",
       "        -0.57195578, -1.02600139, -0.98493871, -1.14714099, -1.11082216]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_instance.model.decision_function([Xval.__getitem__(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "876568e0-a52c-42e3-bec9-fabb7f955f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                              | 85/3342 [00:04<01:57, 27.81it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      " 95%|██████████████████████████████████████████████████████████████████████████    | 3174/3342 [01:19<00:02, 59.50it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3342/3342 [01:23<00:00, 40.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826/1820 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SVM_tester2(CategoriserBase):\n",
    "    def __init__(self):\n",
    "        super(SVM_tester2, self).__init__()\n",
    "    \n",
    "    def precomp(self):\n",
    "        # override\n",
    "        self.model = SVM_instance.model\n",
    "        print('Finished training')\n",
    "    def predict(self, bag):\n",
    "        return self.model.decision_function([bag])[0]\n",
    "    \n",
    "SVM_instance2 = SVM_tester2()\n",
    "SVM_instance2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aa0ffb2-9a4e-482e-b0e9-9812f3965334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                              | 85/3342 [00:05<02:17, 23.74it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      " 95%|██████████████████████████████████████████████████████████████████████████    | 3173/3342 [01:24<00:04, 40.02it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3342/3342 [01:27<00:00, 38.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425/1820 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SVM_tester3(CategoriserBase):\n",
    "    def __init__(self):\n",
    "        super(SVM_tester3, self).__init__()\n",
    "    \n",
    "    def precomp(self):\n",
    "        # override\n",
    "        self.model = svm.LinearSVC(C=5)\n",
    "        self.model.fit(Xtrain, ytrain)\n",
    "        print('Finished training')\n",
    "    def predict(self, bag):\n",
    "        return self.model.decision_function([bag])[0]\n",
    "    \n",
    "SVM_instance3 = SVM_tester3()\n",
    "SVM_instance3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e8e1156-94ba-4e71-813d-d7d81a09692b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n",
      "0.5054484492875104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                              | 85/3342 [00:07<03:20, 16.23it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      " 95%|█████████████████████████████████████████████████████████████████████████▉    | 3170/3342 [01:30<00:03, 57.03it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3342/3342 [01:34<00:00, 35.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740/1820 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive bayes\n",
    "# Note that this doesn't provide benchmark for wholly rejecting an article as irrelevant to every category\n",
    "# This is just for reference\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "class NaiveBayes(CategoriserBase):\n",
    "    def __init__(self):\n",
    "        super(NaiveBayes, self).__init__()\n",
    "    \n",
    "    def precomp(self):\n",
    "        # override\n",
    "        # #assume each category is equally likely so dont fit prior probability (Pr(Class))\n",
    "        self.model = MultinomialNB(fit_prior=False) \n",
    "        self.model.fit(Xtrain, ytrain)\n",
    "        print('Finished training')\n",
    "        print(self.model.score(Xval, yval))\n",
    "    def predict(self, bag):\n",
    "        return self.model.predict_proba([bag])[0]\n",
    "    \n",
    "NaiveBayes_instance = NaiveBayes()\n",
    "NaiveBayes_instance.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3193ed5-ba6f-4df5-b23c-86b8b6ba2f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                              | 85/3342 [00:04<02:03, 26.41it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      " 95%|██████████████████████████████████████████████████████████████████████████    | 3175/3342 [02:18<00:05, 30.38it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3342/3342 [02:24<00:00, 23.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826/1820 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_instance2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d484b08b-efef-4193-a93b-8a36fbde2b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n",
      "0.740989103101425\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'linearRegResults.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-87297156aa49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mLinearReg2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mLinearReg2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"linearRegResults.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, outfile)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"benchmark_results.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"benchmark.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mcases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'linearRegResults.txt'"
     ]
    }
   ],
   "source": [
    "# Try sklearn linearreg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "class LinearReg(CategoriserBase):\n",
    "    def __init__(self):\n",
    "        super(LinearReg, self).__init__()\n",
    "    \n",
    "    def precomp(self):\n",
    "        # override\n",
    "        self.model = LogisticRegression(C=100)\n",
    "        self.model.fit(Xtrain, ytrain)\n",
    "        print('Finished training')\n",
    "        print(self.model.score(Xval, yval))\n",
    "    def predict(self, bag):\n",
    "        return self.model.decision_function([bag])[0]\n",
    "    \n",
    "LinearReg2 = LinearReg()\n",
    "LinearReg2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cea2250-8ab1-40e1-97af-e808d998039d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVM.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(SVM_instance.model, 'SVM.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1dc3061-5f37-464a-aeaa-18fc69198a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89472305, -1.05592616, -0.85802678, -0.620536  , -0.8431903 ,\n",
       "       -0.85703863, -0.34930353, -0.71789778, -0.95431237, -1.09232792])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = load('SVM.joblib') \n",
    "svm_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dfb0955-c13c-443f-b161-8354b52d326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-11.15577666, -11.25522953, -11.6361594 , ..., -11.64587294,\n",
       "        -11.64587294, -11.64587294],\n",
       "       [-10.95581943, -11.10780392, -11.64839949, ..., -11.6491769 ,\n",
       "        -11.6491769 , -11.6491769 ],\n",
       "       [-11.3801378 , -11.55225774, -11.64505718, ..., -11.64563652,\n",
       "        -11.64563652, -11.64563652],\n",
       "       ...,\n",
       "       [-11.50586578, -11.53583265, -11.60427452, ..., -11.64817288,\n",
       "        -11.64817288, -11.64817288],\n",
       "       [-11.5350833 , -11.52855692, -11.61913248, ..., -11.64621433,\n",
       "        -11.64621433, -11.64621433],\n",
       "       [-11.63197237, -11.64201956, -11.64164867, ..., -11.6434185 ,\n",
       "        -11.64044673, -11.64069404]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaiveBayes_instance.model.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc5373fc-77aa-4091-9fb0-08eccc2f2260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "persecution\n",
      "baths\n",
      "mortally\n",
      "1667\n",
      "bec\n",
      "baek\n",
      "b/w\n",
      "klinghoffer\n",
      "azarov\n",
      "capron\n",
      "perpetua\n",
      "biratnagar\n",
      "12.74\n",
      "yaffa\n",
      "cryogenics\n",
      "ef1\n",
      "franchetti\n",
      "blintzes\n",
      "birthstones\n",
      "naadam\n",
      "concertation\n",
      "lesticus\n",
      "containerboard\n",
      "boydston\n",
      "afterellen.com\n",
      "acuff-rose\n",
      "close-fitting\n",
      "packbot\n",
      "comptel\n",
      "tanke\n",
      "saraju\n",
      "rouiba\n",
      "discomfit\n",
      "numurkah\n",
      "hla-a\n",
      "90125\n",
      "zipkin\n",
      "lombarde\n",
      "1.137\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.27773 , -1.8892  ,  0.25371 , -0.60628 ,  1.0393  ,  0.14009 ,\n",
       "       -0.2968  , -0.9551  ,  0.55312 ,  0.30531 ,  0.50993 , -0.26159 ,\n",
       "       -0.033428, -0.41328 ,  0.062837,  0.60586 ,  0.25449 , -0.025989,\n",
       "       -0.43171 , -0.62811 , -1.1064  , -0.10107 , -0.52848 , -0.25303 ,\n",
       "        0.70581 ,  1.0866  ,  0.35819 ,  0.8242  ,  0.75054 ,  1.0691  ,\n",
       "       -0.94066 , -0.86361 ,  1.0516  , -0.42908 , -0.082611, -0.61878 ,\n",
       "       -0.43782 ,  0.16866 , -0.23367 ,  0.35183 , -0.46558 , -0.11573 ,\n",
       "       -0.31309 ,  1.1492  , -1.1831  ,  0.025008,  0.27425 , -0.096663,\n",
       "       -0.012129, -0.22712 ], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Averaging Linear regression\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from nltk import PorterStemmer\n",
    "class Embeddings:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.WEIGHTS_FILE = \"glove50BinWeights.dat\"\n",
    "        self.WORDS_FILE = \"glove50Words.txt\"\n",
    "        self.CUSTOM_WORDLIST = \"corpus.txt\"\n",
    "        self.SIZE = 50\n",
    "        self.NUM_WORDS = 400000\n",
    "        k = open(self.CUSTOM_WORDLIST,\"r\")\n",
    "        self.DIM = 0\n",
    "        self.words = []\n",
    "        while True:\n",
    "            line = k.readline().split()\n",
    "            if len(line) == 2:\n",
    "                self.words.append([line[0], int(line[1])])\n",
    "                self.DIM += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        self.Embeddings = np.fromfile(self.WEIGHTS_FILE, dtype=np.float32).reshape((self.NUM_WORDS,self.SIZE))\n",
    "        self.EmbeddingWords = open(self.WORDS_FILE, encoding=\"utf8\").read().split('\\n')\n",
    "        self.embedIds = {} #unknown chars replaced by '?'\n",
    "        self.stemmedIds = {}\n",
    "        \n",
    "        self.custom_to_embedId = [-1]*self.DIM\n",
    "        stemmer = PorterStemmer()\n",
    "        for i in range(len(self.EmbeddingWords)):\n",
    "            if i%10000 == 0:\n",
    "                print(self.EmbeddingWords[i])\n",
    "            self.embedIds[self.EmbeddingWords[i]] = i\n",
    "            self.stemmedIds[stemmer.stem(self.EmbeddingWords[i])] = i\n",
    "        \n",
    "        for i in range(self.DIM):\n",
    "            self.custom_to_embedId[i] = self.stemmedIds.get(self.words[i][0],-1)\n",
    "            \n",
    "    def get_id_embedding(self, bag_id):\n",
    "        #print(self.words[bag_id][0])\n",
    "        wordId = self.custom_to_embedId[bag_id]\n",
    "        if wordId == -1:\n",
    "            return np.zeros(self.SIZE)\n",
    "        else:\n",
    "            return self.Embeddings[wordId]\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        wordId = self.embedIds.get(word,-1)\n",
    "        if wordId == -1:\n",
    "            return np.zeros(self.SIZE)\n",
    "        else:\n",
    "            return self.Embeddings[wordId]\n",
    "\n",
    "glove50 = Embeddings()\n",
    "glove50.get_id_embedding(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "471d9934-348c-41e8-9230-9b376fd03179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(50, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.Sigmoid() #as opposed to nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class EmbedDataset:\n",
    "    def __init__(self, setting, embedding):\n",
    "        self.FOLDERS = ['History', 'Geography', 'Arts', 'Philosophy_and_religion','Everyday_life',\\\n",
    "                  'Society_and_social_sciences','Biological_and_health_sciences', 'Physical_sciences',\\\n",
    "              'Technology', 'Mathematics']\n",
    "        self.MAP = {}\n",
    "        self.CORPUS_FILE = \"corpus.txt\"\n",
    "        self.SPLIT = np.array([0, 0.7,0.85,1])\n",
    "        self.DIM = 0\n",
    "        self.fileList = []\n",
    "        self.setting = setting # one of 0,1,2 for train, test val\n",
    "        for i in range(len(self.FOLDERS)):\n",
    "            self.MAP[self.FOLDERS[i]] = i\n",
    "        \n",
    "        for folder in self.FOLDERS:\n",
    "            for file in os.listdir(folder):\n",
    "                self.fileList.append((self.MAP[folder], os.path.join(folder, file)))\n",
    "        random.shuffle(self.fileList)\n",
    "        self.SPLIT = [int(c) for c in self.SPLIT * len(self.fileList)]\n",
    "        \n",
    "        k = open(self.CORPUS_FILE,\"r\")\n",
    "        while True:\n",
    "            line = k.readline()\n",
    "            if len(line.split()) == 2:\n",
    "                self.DIM += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        #embedding code\n",
    "        self.embedding = embedding\n",
    "    def __len__(self):\n",
    "        return self.SPLIT[self.setting+1] - self.SPLIT[self.setting]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # we use log bag of words due to zipfian nature\n",
    "        # reads: bag of words file\n",
    "        idx += self.SPLIT[self.setting]\n",
    "        k = open(self.fileList[idx][1], \"r\")\n",
    "        bag = np.zeros(self.embedding.SIZE)\n",
    "        wordcount = 0\n",
    "        while True:\n",
    "            l = k.readline().split()\n",
    "            if len(l) <= 1:\n",
    "                break\n",
    "            l = [int(c) for c in l]\n",
    "            res = self.embedding.get_id_embedding(l[0])\n",
    "            if res.max() != 0 or res.min() != 0:\n",
    "                wordcount += l[1]\n",
    "                bag += l[1]*res\n",
    "        \n",
    "        return torch.tensor(bag/wordcount).float(), self.fileList[idx][0]\n",
    "    \n",
    "trainEmbedder = EmbedDataset(0, glove50)\n",
    "valEmbedder = EmbedDataset(1, glove50)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ec669e26-bdf8-44ec-be2b-0e31cc4ab0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0356, -0.2390, -0.1668, -0.2053, -0.2626, -0.1244,  0.2584,  0.1276,\n",
       "         -0.0043,  0.1877,  0.0490, -0.0356,  0.1766,  0.1438, -0.1229, -0.0610,\n",
       "          0.0473, -0.0268,  0.2997,  0.0906, -0.0253, -0.1005,  0.0223,  0.2159,\n",
       "         -0.0274,  0.4289, -0.0921,  0.0765, -0.0081,  0.0358, -0.3047,  0.0068,\n",
       "          0.1269,  0.0418, -0.0098,  0.0924,  0.0031,  0.0230, -0.2308,  0.0877,\n",
       "          0.0703, -0.3158,  0.1103,  0.2234, -0.0860, -0.0963,  0.1159,  0.1683,\n",
       "         -0.0062, -0.1815]),\n",
       " 1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader = DataLoader(trainEmbedder, batch_size=32, shuffle=True)\n",
    "valLoader = DataLoader(valEmbedder, batch_size=32, shuffle=True)\n",
    "trainEmbedder.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "96de3704-55b7-4ec3-a3fe-9ef587ba7fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=2.3088481426239014\n",
      "Processed 992 examples, cumulative batch loss=71.2272698879242\n",
      "Processed 1952 examples, cumulative batch loss=139.6463007926941\n",
      "Processed 2912 examples, cumulative batch loss=207.41230964660645\n",
      "Processed 3872 examples, cumulative batch loss=274.3126983642578\n",
      "Processed 4832 examples, cumulative batch loss=340.77984404563904\n",
      "Finished epoch, cumulative loss = 391.4669780731201\n",
      "Testset: Correct: 251.0/1193. Cumulative Loss=83.23696422576904\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=2.210111141204834\n",
      "Processed 992 examples, cumulative batch loss=67.65085959434509\n",
      "Processed 1952 examples, cumulative batch loss=132.92929363250732\n",
      "Processed 2912 examples, cumulative batch loss=197.7184443473816\n",
      "Processed 3872 examples, cumulative batch loss=261.6485013961792\n",
      "Processed 4832 examples, cumulative batch loss=325.40186762809753\n",
      "Finished epoch, cumulative loss = 373.5969680547714\n",
      "Testset: Correct: 374.0/1193. Cumulative Loss=79.35733318328857\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=2.0313687324523926\n",
      "Processed 992 examples, cumulative batch loss=64.68868887424469\n",
      "Processed 1952 examples, cumulative batch loss=126.7988578081131\n",
      "Processed 2912 examples, cumulative batch loss=188.41479647159576\n",
      "Processed 3872 examples, cumulative batch loss=249.0893279314041\n",
      "Processed 4832 examples, cumulative batch loss=309.49460458755493\n",
      "Finished epoch, cumulative loss = 355.3035784959793\n",
      "Testset: Correct: 396.0/1193. Cumulative Loss=76.03468561172485\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=1.9644014835357666\n",
      "Processed 992 examples, cumulative batch loss=61.81539237499237\n",
      "Processed 1952 examples, cumulative batch loss=121.17077994346619\n",
      "Processed 2912 examples, cumulative batch loss=180.43966853618622\n",
      "Processed 3872 examples, cumulative batch loss=239.46004784107208\n",
      "Processed 4832 examples, cumulative batch loss=298.42126655578613\n",
      "Finished epoch, cumulative loss = 343.108864068985\n",
      "Testset: Correct: 491.0/1193. Cumulative Loss=73.8564065694809\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=1.9786746501922607\n",
      "Processed 992 examples, cumulative batch loss=60.53196573257446\n",
      "Processed 1952 examples, cumulative batch loss=118.31011235713959\n",
      "Processed 2912 examples, cumulative batch loss=175.7622926235199\n",
      "Processed 3872 examples, cumulative batch loss=233.4464716911316\n",
      "Processed 4832 examples, cumulative batch loss=291.0042008161545\n",
      "Finished epoch, cumulative loss = 334.990988612175\n",
      "Testset: Correct: 553.0/1193. Cumulative Loss=72.24382734298706\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=1.886427879333496\n",
      "Processed 992 examples, cumulative batch loss=59.2571861743927\n",
      "Processed 1952 examples, cumulative batch loss=115.70875668525696\n",
      "Processed 2912 examples, cumulative batch loss=172.73101091384888\n",
      "Processed 3872 examples, cumulative batch loss=229.1260939836502\n",
      "Processed 4832 examples, cumulative batch loss=285.5426312685013\n",
      "Finished epoch, cumulative loss = 328.26599299907684\n",
      "Testset: Correct: 613.0/1193. Cumulative Loss=70.88789248466492\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=1.8853415250778198\n",
      "Processed 992 examples, cumulative batch loss=58.178558468818665\n",
      "Processed 1952 examples, cumulative batch loss=113.60036432743073\n",
      "Processed 2912 examples, cumulative batch loss=169.18343722820282\n",
      "Processed 3872 examples, cumulative batch loss=225.13352501392365\n",
      "Processed 4832 examples, cumulative batch loss=280.013046503067\n",
      "Finished epoch, cumulative loss = 322.5208899974823\n",
      "Testset: Correct: 700.0/1193. Cumulative Loss=69.65008747577667\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=1.802956223487854\n",
      "Processed 992 examples, cumulative batch loss=56.81304669380188\n",
      "Processed 1952 examples, cumulative batch loss=111.77503395080566\n",
      "Processed 2912 examples, cumulative batch loss=166.626362323761\n",
      "Processed 3872 examples, cumulative batch loss=221.34119129180908\n",
      "Processed 4832 examples, cumulative batch loss=275.6504627466202\n",
      "Finished epoch, cumulative loss = 317.29092037677765\n",
      "Testset: Correct: 775.0/1193. Cumulative Loss=68.43246877193451\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=1.8216884136199951\n",
      "Processed 992 examples, cumulative batch loss=56.08300840854645\n",
      "Processed 1952 examples, cumulative batch loss=110.35223591327667\n",
      "Processed 2912 examples, cumulative batch loss=164.16277313232422\n",
      "Processed 3872 examples, cumulative batch loss=217.48848843574524\n",
      "Processed 4832 examples, cumulative batch loss=271.3478115797043\n",
      "Finished epoch, cumulative loss = 312.5461657047272\n",
      "Testset: Correct: 790.0/1193. Cumulative Loss=67.57499408721924\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=1.8208181858062744\n",
      "Processed 992 examples, cumulative batch loss=54.84100127220154\n",
      "Processed 1952 examples, cumulative batch loss=108.2117520570755\n",
      "Processed 2912 examples, cumulative batch loss=161.57148623466492\n",
      "Processed 3872 examples, cumulative batch loss=214.8744068145752\n",
      "Processed 4832 examples, cumulative batch loss=268.12147974967957\n",
      "Finished epoch, cumulative loss = 308.74911546707153\n",
      "Testset: Correct: 818.0/1193. Cumulative Loss=66.7260422706604\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "def trainloop(dataloader, model, loss_fn, optimizer):\n",
    "    current = 0\n",
    "    total_loss = 0\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X=X.to(device)\n",
    "        y=y.to(device)\n",
    "        pred = model(X)\n",
    "        #print(pred)\n",
    "        loss = loss_fn(pred, y)\n",
    "        current += len(X)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 30 == 0:\n",
    "            print(f\"Processed {current} examples, cumulative batch loss={total_loss}\")\n",
    "    print(f\"Finished epoch, cumulative loss = {total_loss}\")\n",
    "\n",
    "def testloop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    print(f\"Testset: Correct: {correct}/{size}. Cumulative Loss={test_loss}\")\n",
    "        \n",
    "# Actual Training\n",
    "epochs = 10 # all you need\n",
    "lr = 0.001\n",
    "model = NeuralNet().to(device) #yolo\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "print(model)\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    trainloop(trainLoader, model, loss, optim)\n",
    "    testloop(valLoader, model, loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5f6ba5ec-d33e-4a55-b114-2253d6b36f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=1.5643229484558105\n",
      "Processed 992 examples, cumulative batch loss=50.34146976470947\n",
      "Processed 1952 examples, cumulative batch loss=98.86135220527649\n",
      "Processed 2912 examples, cumulative batch loss=147.35610222816467\n",
      "Processed 3872 examples, cumulative batch loss=196.09459364414215\n",
      "Processed 4832 examples, cumulative batch loss=244.6925390958786\n",
      "Finished epoch, cumulative loss = 282.1976933479309\n",
      "Testset: Correct: 1002.0/1193. Cumulative Loss=61.0539653301239\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=1.5865298509597778\n",
      "Processed 992 examples, cumulative batch loss=50.668949127197266\n",
      "Processed 1952 examples, cumulative batch loss=99.00928056240082\n",
      "Processed 2912 examples, cumulative batch loss=147.39774656295776\n",
      "Processed 3872 examples, cumulative batch loss=195.71793055534363\n",
      "Processed 4832 examples, cumulative batch loss=244.22700130939484\n",
      "Finished epoch, cumulative loss = 281.10758101940155\n",
      "Testset: Correct: 1009.0/1193. Cumulative Loss=60.73903775215149\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=1.5933364629745483\n",
      "Processed 992 examples, cumulative batch loss=49.607513427734375\n",
      "Processed 1952 examples, cumulative batch loss=98.2142106294632\n",
      "Processed 2912 examples, cumulative batch loss=146.5843312740326\n",
      "Processed 3872 examples, cumulative batch loss=195.07018625736237\n",
      "Processed 4832 examples, cumulative batch loss=243.12761425971985\n",
      "Finished epoch, cumulative loss = 280.19287300109863\n",
      "Testset: Correct: 1010.0/1193. Cumulative Loss=60.53998517990112\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=1.5668210983276367\n",
      "Processed 992 examples, cumulative batch loss=49.461029410362244\n",
      "Processed 1952 examples, cumulative batch loss=97.50002789497375\n",
      "Processed 2912 examples, cumulative batch loss=146.06336581707\n",
      "Processed 3872 examples, cumulative batch loss=194.32954025268555\n",
      "Processed 4832 examples, cumulative batch loss=242.64486408233643\n",
      "Finished epoch, cumulative loss = 279.6861392259598\n",
      "Testset: Correct: 1000.0/1193. Cumulative Loss=60.59677791595459\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=1.6258479356765747\n",
      "Processed 992 examples, cumulative batch loss=49.8552862405777\n",
      "Processed 1952 examples, cumulative batch loss=97.82696866989136\n",
      "Processed 2912 examples, cumulative batch loss=146.11891269683838\n",
      "Processed 3872 examples, cumulative batch loss=194.29999268054962\n",
      "Processed 4832 examples, cumulative batch loss=242.19441139698029\n",
      "Finished epoch, cumulative loss = 279.1056077480316\n",
      "Testset: Correct: 1016.0/1193. Cumulative Loss=60.4708731174469\n"
     ]
    }
   ],
   "source": [
    "epochs = 5 # all you need\n",
    "lr = 0.01\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "print(model)\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    trainloop(trainLoader, model, loss, optim)\n",
    "    testloop(valLoader, model, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "3d0ed949-4f7f-4754-b119-69422d35fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt):\n",
    "    txt = txt.lower()\n",
    "    ALLOWED = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "    cleaned = \"\"\n",
    "    for i in range(len(txt)):\n",
    "        if txt[i] in ALLOWED:\n",
    "            cleaned += txt[i]\n",
    "        else:\n",
    "            cleaned += \" \"\n",
    "    return cleaned\n",
    "STOP_WORDS = [\"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\",\\\n",
    "              \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\",\\\n",
    "              \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\",\\\n",
    "              \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\",\\\n",
    "              \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\",\\\n",
    "              \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\",\\\n",
    "              \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\",\\\n",
    "              \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\",\\\n",
    "              \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\",\\\n",
    "              \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\",\\\n",
    "              \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\",\\\n",
    "              \"further\", \"was\", \"here\", \"than\"]\n",
    "\n",
    "\n",
    "\n",
    "def test(model, embedding, txt):\n",
    "    x = clean(txt).split()\n",
    "    averaged_embedding = np.zeros(embedding.SIZE)\n",
    "    word_count = 0\n",
    "    for word in x:\n",
    "        if word.lower() in STOP_WORDS:\n",
    "            continue\n",
    "        res = embedding.get_word_embedding(word.lower())\n",
    "        if res.max() != 0 or res.min() != 0:\n",
    "            averaged_embedding += res\n",
    "            word_count += 1\n",
    "    averaged_embedding /= word_count\n",
    "    return model(torch.tensor(averaged_embedding).float().unsqueeze(0))[0]\n",
    "\n",
    "def eval_embeddings(model, embedding, testfunc, outfile=\"benchmark_results.txt\", top=2, idle=False, size=-1):\n",
    "        file = open(\"benchmark.txt\",\"r\").read().split('\\n')\n",
    "        BENCHMARK_BINARIES = \"benchmark_binaries\"\n",
    "        CLASSES = ['History', 'Geography', 'Arts', 'Philosophy_and_religion','Everyday_life',\\\n",
    "                  'Society_and_social_sciences','Biological_and_health_sciences', 'Physical_sciences',\\\n",
    "              'Technology', 'Mathematics']\n",
    "        try: checkpoint = open(outfile,\"r\").read()\n",
    "        except: checkpoint = \"\"\n",
    "        cases = 0\n",
    "        correct = 0\n",
    "        \n",
    "        if size == -1:\n",
    "            size = len(file)\n",
    "        it = range(size)\n",
    "        if not idle:\n",
    "            it = tqdm(it)\n",
    "        for testId in it:\n",
    "            try: [ans, url] = file[testId].split()\n",
    "            except: continue\n",
    "\n",
    "            txt_file_name = \"benchmark/test\"+str(testId)+\".txt\"\n",
    "            outfile_string = \"Test \"+str(testId)+\": \"\n",
    "            try:\n",
    "                raw_txt = open(txt_file_name, \"r\").read()\n",
    "                if len(raw_txt) <= 10:\n",
    "                    continue\n",
    "                res = testfunc(model, embedding, raw_txt)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            cases += 1\n",
    "            #print(ans, url)\n",
    "            res = [float(res[x]) for x in range(10)]\n",
    "            if cases % 50 == 0 and idle:\n",
    "                 print(f\"Processed {cases} Cases\")\n",
    "            \n",
    "            correct_label = CLASSES.index(ans)\n",
    "            # Metrics:\n",
    "            # We consider a result correct if it exceeds 0 and falls in the top 2 confidences\n",
    "            # Optional: CHeck for seperation but meh\n",
    "\n",
    "            # Check exact category score\n",
    "            sorted_res = sorted(res)\n",
    "            \n",
    "            if res[correct_label] >= sorted_res[-top]:\n",
    "                correct += 1\n",
    "\n",
    "            checkpoint += outfile_string + \" \".join([str(c) for c in res])+\"\\n\"\n",
    "        print(f\"{correct}/{cases} correct\")\n",
    "        output_file = open(outfile,\"w\")\n",
    "        output_file.write(checkpoint)\n",
    "        output_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d0f85-ed84-4c98-91cd-aa6329e56764",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_embeddings(model, glove50,test, outfile=\"benchmark_naive_embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "89f73f50-01ee-4e17-8c74-fff13fdc45cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=2.309086799621582\n",
      "Processed 992 examples, cumulative batch loss=64.53204190731049\n",
      "Processed 1952 examples, cumulative batch loss=120.62792348861694\n",
      "Processed 2912 examples, cumulative batch loss=174.5756415128708\n",
      "Processed 3872 examples, cumulative batch loss=227.83879125118256\n",
      "Processed 4832 examples, cumulative batch loss=280.4299737215042\n",
      "Finished epoch, cumulative loss = 320.0798736810684\n",
      "Testset: Correct: 881.0/1193. Cumulative Loss=65.96121454238892\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=1.7390564680099487\n",
      "Processed 992 examples, cumulative batch loss=54.06024503707886\n",
      "Processed 1952 examples, cumulative batch loss=105.8529691696167\n",
      "Processed 2912 examples, cumulative batch loss=157.49269759655\n",
      "Processed 3872 examples, cumulative batch loss=208.76527154445648\n",
      "Processed 4832 examples, cumulative batch loss=260.01323556900024\n",
      "Finished epoch, cumulative loss = 298.7789763212204\n",
      "Testset: Correct: 945.0/1193. Cumulative Loss=63.653284549713135\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=1.6922147274017334\n",
      "Processed 992 examples, cumulative batch loss=52.358081579208374\n",
      "Processed 1952 examples, cumulative batch loss=102.33546996116638\n",
      "Processed 2912 examples, cumulative batch loss=152.32930505275726\n",
      "Processed 3872 examples, cumulative batch loss=202.7036269903183\n",
      "Processed 4832 examples, cumulative batch loss=253.0495468378067\n",
      "Finished epoch, cumulative loss = 291.3050801753998\n",
      "Testset: Correct: 967.0/1193. Cumulative Loss=63.240806102752686\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=1.6417591571807861\n",
      "Processed 992 examples, cumulative batch loss=51.295653223991394\n",
      "Processed 1952 examples, cumulative batch loss=101.62134158611298\n",
      "Processed 2912 examples, cumulative batch loss=151.23371481895447\n",
      "Processed 3872 examples, cumulative batch loss=201.01470518112183\n",
      "Processed 4832 examples, cumulative batch loss=251.0018664598465\n",
      "Finished epoch, cumulative loss = 289.60993242263794\n",
      "Testset: Correct: 958.0/1193. Cumulative Loss=62.712286710739136\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=1.7173593044281006\n",
      "Processed 992 examples, cumulative batch loss=51.62269175052643\n",
      "Processed 1952 examples, cumulative batch loss=101.5077555179596\n",
      "Processed 2912 examples, cumulative batch loss=151.34897541999817\n",
      "Processed 3872 examples, cumulative batch loss=201.1288356781006\n",
      "Processed 4832 examples, cumulative batch loss=250.74249255657196\n",
      "Finished epoch, cumulative loss = 288.7794510126114\n",
      "Testset: Correct: 965.0/1193. Cumulative Loss=62.40486657619476\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=1.6908485889434814\n",
      "Processed 992 examples, cumulative batch loss=50.90115308761597\n",
      "Processed 1952 examples, cumulative batch loss=100.57939267158508\n",
      "Processed 2912 examples, cumulative batch loss=150.28197753429413\n",
      "Processed 3872 examples, cumulative batch loss=199.69496965408325\n",
      "Processed 4832 examples, cumulative batch loss=249.69162344932556\n",
      "Finished epoch, cumulative loss = 287.93353247642517\n",
      "Testset: Correct: 971.0/1193. Cumulative Loss=62.62468898296356\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=1.6494276523590088\n",
      "Processed 992 examples, cumulative batch loss=51.37935125827789\n",
      "Processed 1952 examples, cumulative batch loss=101.0660594701767\n",
      "Processed 2912 examples, cumulative batch loss=150.43191063404083\n",
      "Processed 3872 examples, cumulative batch loss=200.14046216011047\n",
      "Processed 4832 examples, cumulative batch loss=249.90902709960938\n",
      "Finished epoch, cumulative loss = 287.6943607330322\n",
      "Testset: Correct: 970.0/1193. Cumulative Loss=62.67760467529297\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=1.7063742876052856\n",
      "Processed 992 examples, cumulative batch loss=51.10994899272919\n",
      "Processed 1952 examples, cumulative batch loss=100.6037905216217\n",
      "Processed 2912 examples, cumulative batch loss=150.12473475933075\n",
      "Processed 3872 examples, cumulative batch loss=199.54654133319855\n",
      "Processed 4832 examples, cumulative batch loss=249.01715290546417\n",
      "Finished epoch, cumulative loss = 287.10749554634094\n",
      "Testset: Correct: 976.0/1193. Cumulative Loss=62.25436508655548\n",
      "Epoch 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-85f6911d0ac2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {i}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelSimple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mtestloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelSimple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-c09fc9fc0b1b>\u001b[0m in \u001b[0;36mtrainloop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-04427fa487ea>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_id_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                 \u001b[0mwordcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mbag\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     37\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[0;32m     38\u001b[0m           initial=_NoValue, where=True):\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(50, 10),\n",
    "            nn.Sigmoid() #as opposed to nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "#simpler model\n",
    "epochs = 15 \n",
    "lr = 0.1\n",
    "modelSimple = LogisticRegression()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(modelSimple.parameters(), lr = lr, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "print(modelSimple)\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    trainloop(trainLoader, modelSimple, loss, optim)\n",
    "    testloop(valLoader, modelSimple, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b4cac-f331-4bd3-b4fc-c73741802c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 layer network?\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DeeperNeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DeeperNeuralNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(50, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.Sigmoid() #as opposed to nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    #simpler model\n",
    "epochs = 15 \n",
    "lr = 0.01\n",
    "modelComplex = DeeperNeuralNet()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(modelComplex.parameters(), lr = lr, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "print(modelComplex)\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    trainloop(trainLoader, modelComplex, loss, optim)\n",
    "    testloop(valLoader, modelComplex, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "77e2ec2f-b253-4ae5-8bff-38f3b3aa80fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████▌               | 2679/3342 [00:27<00:10, 60.82it/s]<ipython-input-341-0e98480052b6>:37: RuntimeWarning: invalid value encountered in true_divide\n",
      "  averaged_embedding /= word_count\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3342/3342 [00:32<00:00, 101.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272/1822 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_embeddings(modelSimple, glove50, test, outfile=\"benchmark_naive_embedding_regression2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "14e5da57-9be4-4584-9b8f-58b211f5e1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                               | 42/3342 [00:03<05:29, 10.01it/s]<ipython-input-341-0e98480052b6>:37: RuntimeWarning: invalid value encountered in true_divide\n",
      "  averaged_embedding /= word_count\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3342/3342 [01:02<00:00, 53.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365/1822 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_sentence_average(model, embedding, txt):\n",
    "    txt = txt.split(' ') \n",
    "    res = torch.zeros(10)\n",
    "    WORDS_PER_SENTENCE = 40\n",
    "    pt = 0\n",
    "    while pt < len(txt):\n",
    "        sentence = ' '.join(txt[pt:pt+WORDS_PER_SENTENCE])\n",
    "        if len(sentence) >= 40: \n",
    "            tmp = test(model, embedding, sentence)\n",
    "            if tmp.max() > 0.3:\n",
    "                order = tmp.argsort()\n",
    "                res[order[-1]] += 1\n",
    "        pt += 30\n",
    "    #print(res)\n",
    "    return res\n",
    "\n",
    "eval_embeddings(modelSimple, glove50, test_sentence_average, outfile=\"benchmark_naive_sentenceaveraged_embedding_regression2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "7e40bb0c-fa2b-4725-a526-ba19cdbbd5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7953/7953 [00:24<00:00, 324.76it/s]\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayesWeighting:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.FOLDERS = ['History', 'Geography', 'Arts', 'Philosophy_and_religion','Everyday_life',\\\n",
    "                  'Society_and_social_sciences','Biological_and_health_sciences', 'Physical_sciences',\\\n",
    "              'Technology', 'Mathematics']\n",
    "        self.MAP = {}\n",
    "        self.CORPUS_FILE = \"corpus.txt\"\n",
    "        self.DIM = 0\n",
    "        self.fileList = []\n",
    "        for i in range(len(self.FOLDERS)):\n",
    "            self.MAP[self.FOLDERS[i]] = i\n",
    "        \n",
    "        for folder in self.FOLDERS:\n",
    "            for file in os.listdir(folder):\n",
    "                self.fileList.append((self.MAP[folder], os.path.join(folder, file)))\n",
    "        random.shuffle(self.fileList)\n",
    "\n",
    "        \n",
    "        k = open(self.CORPUS_FILE,\"r\")\n",
    "        \n",
    "        self.freq = []\n",
    "        self.word_list = {} #word to id\n",
    "        while True:\n",
    "            line = k.readline().split()\n",
    "            if len(line) == 2:\n",
    "                self.freq.append(int(line[1]))\n",
    "                self.word_list[line[0]]=self.DIM\n",
    "                self.DIM += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        self.total_length = 0\n",
    "        self.length_counts = [0]*len(self.FOLDERS)\n",
    "        self.positive = [[0]*len(self.FOLDERS) for i in range(self.DIM)]\n",
    "        self.stemmer = PorterStemmer()\n",
    "        for file in tqdm(self.fileList):\n",
    "            k = open(file[1], \"r\").read().split('\\n')\n",
    "            for line in k:\n",
    "                if len(line) <= 2:\n",
    "                    break\n",
    "                [wordId, num] = [int(c) for c in line.split()]\n",
    "                self.length_counts[file[0]] += num\n",
    "                self.positive[wordId][file[0]] += num\n",
    "                self.total_length += num\n",
    "        \n",
    "    def id_to_weight(self, wid, classId):\n",
    "        #laplacian smoothing\n",
    "        negative_len = self.total_length - self.length_counts[classId] + self.DIM\n",
    "        positive_len = self.length_counts[classId] + self.DIM\n",
    "        positive_freq = self.positive[wid][classId]\n",
    "        negative_freq = self.freq[wid]-positive_freq\n",
    "        positive_freq +=1 \n",
    "        negative_freq +=1 #laplace smooth\n",
    "        return np.log(positive_freq/positive_len)-np.log(negative_freq/negative_len)\n",
    "    \n",
    "    def word_to_weight(self, word, classId):\n",
    "        stemmed = self.stemmer.stem(word.lower())\n",
    "        id = self.word_list.get(stemmed,-1)\n",
    "        if id != -1:\n",
    "            return self.id_to_weight(id, classId)\n",
    "        else:\n",
    "            negative_len = self.total_length - self.length_counts[classId] + self.DIM\n",
    "            positive_len = self.length_counts[classId] + self.DIM\n",
    "            return np.log(negative_len)-np.log(positive_len) # both freqs are 0\n",
    "weightScheme = NaiveBayesWeighting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5b9d7673-c54c-43c1-841c-59c96fffc76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113735 41989465 [5634288, 8313612, 3441379, 2729219, 2249242, 4962694, 6063669, 4226604, 3357972, 1010786]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weightScheme.DIM, weightScheme.total_length, weightScheme.length_counts)\n",
    "weightScheme.word_to_weight(\"QaQ\",1)\n",
    "weightScheme.id_to_weight(15197,0)\n",
    "weightScheme.positive[15197][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "dc717630-6162-4911-ad50-0e9bee0ebe43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3468"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeightedEmbedDataset:\n",
    "    def __init__(self, setting, embedding, class_id, SAMPLE_FACTOR=3):\n",
    "        self.FOLDERS = ['History', 'Geography', 'Arts', 'Philosophy_and_religion','Everyday_life',\\\n",
    "                  'Society_and_social_sciences','Biological_and_health_sciences', 'Physical_sciences',\\\n",
    "              'Technology', 'Mathematics']\n",
    "        self.MAP = {}\n",
    "        self.CORPUS_FILE = \"corpus.txt\"\n",
    "        self.SPLIT = np.array([0, 0.85, 1])\n",
    "        self.DIM = 0\n",
    "        self.fileList = []\n",
    "        self.setting = setting # one of 0,1,2 for train, test val\n",
    "        self.class_id = class_id\n",
    "        for i in range(len(self.FOLDERS)):\n",
    "            self.MAP[self.FOLDERS[i]] = i\n",
    "        \n",
    "        #\"\"\"\n",
    "        num_cases = SAMPLE_FACTOR*len(os.listdir(self.FOLDERS[class_id]))\n",
    "        #negative examples\n",
    "        for folder in self.FOLDERS:\n",
    "            if self.MAP[folder] != class_id:\n",
    "                for file in os.listdir(folder):\n",
    "                    self.fileList.append((self.MAP[folder], os.path.join(folder, file)))\n",
    "        \n",
    "        random.shuffle(self.fileList)\n",
    "        self.fileList = self.fileList[:num_cases]\n",
    "        \n",
    "        #positive examples\n",
    "        for file in os.listdir(self.FOLDERS[class_id]):\n",
    "            for i in range(SAMPLE_FACTOR):\n",
    "                self.fileList.append((class_id, os.path.join(self.FOLDERS[class_id], file)))\n",
    "            \n",
    "        random.shuffle(self.fileList)\n",
    "        #\"\"\" \n",
    "        \n",
    "        # load all \n",
    "        #for folder in self.FOLDERS:\n",
    "        #    for file in os.listdir(folder):\n",
    "        #        self.fileList.append((self.MAP[folder], os.path.join(folder, file)))\n",
    "        #        \n",
    "        self.SPLIT = [int(c) for c in self.SPLIT * len(self.fileList)]\n",
    "        \n",
    "        k = open(self.CORPUS_FILE,\"r\")\n",
    "        while True:\n",
    "            line = k.readline()\n",
    "            if len(line.split()) == 2:\n",
    "                self.DIM += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        #embedding code\n",
    "        self.embedding = embedding\n",
    "    def __len__(self):\n",
    "        return self.SPLIT[self.setting+1] - self.SPLIT[self.setting]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # we use log bag of words due to zipfian nature\n",
    "        # reads: bag of words file\n",
    "        idx += self.SPLIT[self.setting]\n",
    "        k = open(self.fileList[idx][1], \"r\")\n",
    "        bag = np.zeros(self.embedding.SIZE)\n",
    "        wordcount = 0\n",
    "        #print(self.fileList[idx][1])\n",
    "        while True:\n",
    "            l = k.readline().split()\n",
    "            if len(l) <= 1:\n",
    "                break\n",
    "            l = [int(c) for c in l]\n",
    "            res = self.embedding.get_id_embedding(l[0])\n",
    "            if res.max() != 0 or res.min() != 0:\n",
    "                wt = weightScheme.id_to_weight(l[0], self.class_id)\n",
    "                wordcount += l[1]\n",
    "                bag += l[1]*res*wt\n",
    "        #print(bag, wordcount)\n",
    "        return torch.tensor(bag/wordcount).float(), int(self.fileList[idx][0] == self.class_id)\n",
    "WEset = WeightedEmbedDataset(0, glove50, i)\n",
    "WEset.__getitem__(1)\n",
    "WEset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4f12f518-612f-4923-ba66-8b3bedd1c7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.6812331080436707\n",
      "Processed 992 examples, cumulative batch loss=20.000382125377655\n",
      "Processed 1952 examples, cumulative batch loss=37.59262067079544\n",
      "Finished epoch, cumulative loss = 53.48525208234787\n",
      "Testset: Correct: 551.0/612. Cumulative Loss=10.699474334716797\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.5099274516105652\n",
      "Processed 992 examples, cumulative batch loss=16.0888774394989\n",
      "Processed 1952 examples, cumulative batch loss=31.079753816127777\n",
      "Finished epoch, cumulative loss = 45.06666058301926\n",
      "Testset: Correct: 551.0/612. Cumulative Loss=9.498610854148865\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.46167173981666565\n",
      "Processed 992 examples, cumulative batch loss=14.680695444345474\n",
      "Processed 1952 examples, cumulative batch loss=28.56091609597206\n",
      "Finished epoch, cumulative loss = 41.69656214118004\n",
      "Testset: Correct: 553.0/612. Cumulative Loss=9.10963624715805\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.4993407428264618\n",
      "Processed 992 examples, cumulative batch loss=13.965082824230194\n",
      "Processed 1952 examples, cumulative batch loss=27.343662589788437\n",
      "Finished epoch, cumulative loss = 40.104236513376236\n",
      "Testset: Correct: 555.0/612. Cumulative Loss=8.759713560342789\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.40370622277259827\n",
      "Processed 992 examples, cumulative batch loss=13.615100592374802\n",
      "Processed 1952 examples, cumulative batch loss=26.63020622730255\n",
      "Finished epoch, cumulative loss = 39.02254047989845\n",
      "Testset: Correct: 555.0/612. Cumulative Loss=8.610728174448013\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.4298560619354248\n",
      "Processed 992 examples, cumulative batch loss=13.313133329153061\n",
      "Processed 1952 examples, cumulative batch loss=26.007730722427368\n",
      "Finished epoch, cumulative loss = 38.333835780620575\n",
      "Testset: Correct: 557.0/612. Cumulative Loss=8.471568018198013\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.42584705352783203\n",
      "Processed 992 examples, cumulative batch loss=13.006902545690536\n",
      "Processed 1952 examples, cumulative batch loss=25.58730047941208\n",
      "Finished epoch, cumulative loss = 37.88239386677742\n",
      "Testset: Correct: 557.0/612. Cumulative Loss=8.423603415489197\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.4597037136554718\n",
      "Processed 992 examples, cumulative batch loss=12.876772731542587\n",
      "Processed 1952 examples, cumulative batch loss=25.55469435453415\n",
      "Finished epoch, cumulative loss = 37.43627691268921\n",
      "Testset: Correct: 557.0/612. Cumulative Loss=8.39316800236702\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.37246376276016235\n",
      "Processed 992 examples, cumulative batch loss=12.933798879384995\n",
      "Processed 1952 examples, cumulative batch loss=25.24174678325653\n",
      "Finished epoch, cumulative loss = 37.18467661738396\n",
      "Testset: Correct: 557.0/612. Cumulative Loss=8.255652040243149\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.47531309723854065\n",
      "Processed 992 examples, cumulative batch loss=12.915635794401169\n",
      "Processed 1952 examples, cumulative batch loss=25.11013400554657\n",
      "Finished epoch, cumulative loss = 36.94419023394585\n",
      "Testset: Correct: 558.0/612. Cumulative Loss=8.36025658249855\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.6875787377357483\n",
      "Processed 992 examples, cumulative batch loss=20.144962191581726\n",
      "Processed 1952 examples, cumulative batch loss=37.767357647418976\n",
      "Processed 2912 examples, cumulative batch loss=53.943696558475494\n",
      "Processed 3872 examples, cumulative batch loss=69.45968744158745\n",
      "Processed 4832 examples, cumulative batch loss=84.36361029744148\n",
      "Finished epoch, cumulative loss = 87.62786358594894\n",
      "Testset: Correct: 1016.0/1082. Cumulative Loss=16.33610600233078\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.46795549988746643\n",
      "Processed 992 examples, cumulative batch loss=14.612503290176392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-279-7d9ff76dc836>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {i}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mtrainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWEtrainLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mtestloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWEtestLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-c09fc9fc0b1b>\u001b[0m in \u001b[0;36mtrainloop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\junhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-277-db5ff21f109f>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mwt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweightScheme\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid_to_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mwordcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mbag\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;31m#print(bag, wordcount)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mwordcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(50, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "nbmodels = [BinaryClassifier() for i in range(10)]\n",
    "# Actual Training\n",
    "for i in range(1,10):\n",
    "    epochs = 10 # all you need\n",
    "    lr = 0.003 #can probs do 0.1\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    nbmodels[i].to(device)\n",
    "    optim = torch.optim.AdamW(nbmodels[i].parameters(), lr = lr, weight_decay = 0.01)\n",
    "    WEtrainLoader = DataLoader(WeightedEmbedDataset(0, glove50, i), batch_size=32, shuffle=True)\n",
    "    WEtestLoader = DataLoader(WeightedEmbedDataset(1, glove50, i), batch_size=32, shuffle=True)\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        print(f\"Epoch {j}\")\n",
    "        trainloop(WEtrainLoader, nbmodels[i], loss, optim)\n",
    "        testloop(WEtestLoader, nbmodels[i], loss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "be5ff28c-0e7f-4011-8a44-194baed114a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier 0\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.688480019569397\n",
      "Processed 992 examples, cumulative batch loss=18.32465636730194\n",
      "Finished epoch, cumulative loss = 21.358395010232925\n",
      "Testset: Correct: 187.0/204. Cumulative Loss=3.4554097950458527\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.49270662665367126\n",
      "Processed 992 examples, cumulative batch loss=14.83844980597496\n",
      "Finished epoch, cumulative loss = 17.593986302614212\n",
      "Testset: Correct: 187.0/204. Cumulative Loss=3.0799742937088013\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.4957418441772461\n",
      "Processed 992 examples, cumulative batch loss=13.888905942440033\n",
      "Finished epoch, cumulative loss = 16.494646579027176\n",
      "Testset: Correct: 187.0/204. Cumulative Loss=3.0056861639022827\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.3954540193080902\n",
      "Processed 992 examples, cumulative batch loss=13.270605951547623\n",
      "Finished epoch, cumulative loss = 15.983509451150894\n",
      "Testset: Correct: 186.0/204. Cumulative Loss=2.948674291372299\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.36561405658721924\n",
      "Processed 992 examples, cumulative batch loss=13.170425117015839\n",
      "Finished epoch, cumulative loss = 15.613713502883911\n",
      "Testset: Correct: 186.0/204. Cumulative Loss=2.873020887374878\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.3856361508369446\n",
      "Processed 992 examples, cumulative batch loss=13.068726152181625\n",
      "Finished epoch, cumulative loss = 15.38670563697815\n",
      "Testset: Correct: 186.0/204. Cumulative Loss=2.838238835334778\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.3878180682659149\n",
      "Processed 992 examples, cumulative batch loss=12.765861183404922\n",
      "Finished epoch, cumulative loss = 15.267436474561691\n",
      "Testset: Correct: 186.0/204. Cumulative Loss=2.8078171014785767\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.4185028672218323\n",
      "Processed 992 examples, cumulative batch loss=12.612092465162277\n",
      "Finished epoch, cumulative loss = 15.348440498113632\n",
      "Testset: Correct: 187.0/204. Cumulative Loss=2.842949479818344\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.35596519708633423\n",
      "Processed 992 examples, cumulative batch loss=12.704303205013275\n",
      "Finished epoch, cumulative loss = 15.034715294837952\n",
      "Testset: Correct: 186.0/204. Cumulative Loss=2.8650926649570465\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.4088461697101593\n",
      "Processed 992 examples, cumulative batch loss=12.52919015288353\n",
      "Finished epoch, cumulative loss = 14.956847846508026\n",
      "Testset: Correct: 186.0/204. Cumulative Loss=2.778222918510437\n",
      "Training classifier 1\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.704888105392456\n",
      "Processed 992 examples, cumulative batch loss=18.48739606142044\n",
      "Processed 1952 examples, cumulative batch loss=32.65683400630951\n",
      "Finished epoch, cumulative loss = 33.94432407617569\n",
      "Testset: Correct: 339.0/361. Cumulative Loss=5.403427243232727\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.46379169821739197\n",
      "Processed 992 examples, cumulative batch loss=13.440772593021393\n",
      "Processed 1952 examples, cumulative batch loss=25.83871066570282\n",
      "Finished epoch, cumulative loss = 27.022734194993973\n",
      "Testset: Correct: 342.0/361. Cumulative Loss=4.9155367612838745\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.3930738568305969\n",
      "Processed 992 examples, cumulative batch loss=12.509706109762192\n",
      "Processed 1952 examples, cumulative batch loss=24.309397876262665\n",
      "Finished epoch, cumulative loss = 25.4777569770813\n",
      "Testset: Correct: 343.0/361. Cumulative Loss=4.7097883224487305\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.39549198746681213\n",
      "Processed 992 examples, cumulative batch loss=12.087822407484055\n",
      "Processed 1952 examples, cumulative batch loss=23.584422945976257\n",
      "Finished epoch, cumulative loss = 24.742847979068756\n",
      "Testset: Correct: 344.0/361. Cumulative Loss=4.602568030357361\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.39393186569213867\n",
      "Processed 992 examples, cumulative batch loss=11.82126197218895\n",
      "Processed 1952 examples, cumulative batch loss=23.18373030424118\n",
      "Finished epoch, cumulative loss = 24.29221299290657\n",
      "Testset: Correct: 346.0/361. Cumulative Loss=4.561256617307663\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.3548814654350281\n",
      "Processed 992 examples, cumulative batch loss=11.52799329161644\n",
      "Processed 1952 examples, cumulative batch loss=22.84017300605774\n",
      "Finished epoch, cumulative loss = 23.992048025131226\n",
      "Testset: Correct: 346.0/361. Cumulative Loss=4.448715329170227\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.3762190341949463\n",
      "Processed 992 examples, cumulative batch loss=11.455712795257568\n",
      "Processed 1952 examples, cumulative batch loss=22.711273729801178\n",
      "Finished epoch, cumulative loss = 23.755580246448517\n",
      "Testset: Correct: 346.0/361. Cumulative Loss=4.404248505830765\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.35586273670196533\n",
      "Processed 992 examples, cumulative batch loss=11.330973416566849\n",
      "Processed 1952 examples, cumulative batch loss=22.465698689222336\n",
      "Finished epoch, cumulative loss = 23.58117124438286\n",
      "Testset: Correct: 347.0/361. Cumulative Loss=4.369626998901367\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.35917574167251587\n",
      "Processed 992 examples, cumulative batch loss=11.481641978025436\n",
      "Processed 1952 examples, cumulative batch loss=22.328641206026077\n",
      "Finished epoch, cumulative loss = 23.4412579536438\n",
      "Testset: Correct: 348.0/361. Cumulative Loss=4.372235834598541\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.33224421739578247\n",
      "Processed 992 examples, cumulative batch loss=11.247168451547623\n",
      "Processed 1952 examples, cumulative batch loss=22.243084222078323\n",
      "Finished epoch, cumulative loss = 23.321293979883194\n",
      "Testset: Correct: 349.0/361. Cumulative Loss=4.362479776144028\n",
      "Training classifier 2\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.6911417841911316\n",
      "Processed 992 examples, cumulative batch loss=18.045275688171387\n",
      "Finished epoch, cumulative loss = 20.553380489349365\n",
      "Testset: Correct: 196.0/201. Cumulative Loss=3.2959281504154205\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.501798689365387\n",
      "Processed 992 examples, cumulative batch loss=14.122170746326447\n",
      "Finished epoch, cumulative loss = 16.29030653834343\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.976532995700836\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.41994163393974304\n",
      "Processed 992 examples, cumulative batch loss=12.858389914035797\n",
      "Finished epoch, cumulative loss = 14.932598680257797\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.7957388758659363\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.4031098783016205\n",
      "Processed 992 examples, cumulative batch loss=12.366287589073181\n",
      "Finished epoch, cumulative loss = 14.256705671548843\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.7616658210754395\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.37525394558906555\n",
      "Processed 992 examples, cumulative batch loss=11.967233628034592\n",
      "Finished epoch, cumulative loss = 13.866518408060074\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.634370803833008\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.3771795332431793\n",
      "Processed 992 examples, cumulative batch loss=11.683950275182724\n",
      "Finished epoch, cumulative loss = 13.612967401742935\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.5867307782173157\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.3972618281841278\n",
      "Processed 992 examples, cumulative batch loss=11.56004324555397\n",
      "Finished epoch, cumulative loss = 13.38650381565094\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.5705247819423676\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.3645099997520447\n",
      "Processed 992 examples, cumulative batch loss=11.430090844631195\n",
      "Finished epoch, cumulative loss = 13.24491548538208\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.555584639310837\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.365400493144989\n",
      "Processed 992 examples, cumulative batch loss=11.353663265705109\n",
      "Finished epoch, cumulative loss = 13.122905731201172\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.5101696848869324\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.34681835770606995\n",
      "Processed 992 examples, cumulative batch loss=11.174873769283295\n",
      "Finished epoch, cumulative loss = 13.030550926923752\n",
      "Testset: Correct: 195.0/201. Cumulative Loss=2.4995618760585785\n",
      "Training classifier 3\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.6842504143714905\n",
      "Finished epoch, cumulative loss = 13.700571835041046\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=2.405227482318878\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.4812883138656616\n",
      "Finished epoch, cumulative loss = 11.078618973493576\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=2.086858570575714\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.4111523926258087\n",
      "Finished epoch, cumulative loss = 9.661212682723999\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=1.9569452702999115\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.3898216485977173\n",
      "Finished epoch, cumulative loss = 9.253135085105896\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=1.847342610359192\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.36889225244522095\n",
      "Finished epoch, cumulative loss = 9.172908902168274\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=1.8303622007369995\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.362662672996521\n",
      "Finished epoch, cumulative loss = 8.809554994106293\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=1.7959398329257965\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.3837661147117615\n",
      "Finished epoch, cumulative loss = 8.694429636001587\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=1.806542545557022\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.34392914175987244\n",
      "Finished epoch, cumulative loss = 8.600065499544144\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=1.746521383523941\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.35266968607902527\n",
      "Finished epoch, cumulative loss = 8.538653641939163\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=1.741269052028656\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.3374125361442566\n",
      "Finished epoch, cumulative loss = 8.476736634969711\n",
      "Testset: Correct: 128.0/131. Cumulative Loss=1.7370356619358063\n",
      "Training classifier 4\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.7108811736106873\n",
      "Finished epoch, cumulative loss = 16.018006026744843\n",
      "Testset: Correct: 143.0/144. Cumulative Loss=2.629249334335327\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.5429216027259827\n",
      "Finished epoch, cumulative loss = 12.824575871229172\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=2.223632723093033\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.449026495218277\n",
      "Finished epoch, cumulative loss = 11.521557927131653\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=2.0556114315986633\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.41218316555023193\n",
      "Finished epoch, cumulative loss = 10.826846331357956\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=1.9336382150650024\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.41792574524879456\n",
      "Finished epoch, cumulative loss = 10.466915786266327\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=1.8735967576503754\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.38088372349739075\n",
      "Finished epoch, cumulative loss = 10.212581008672714\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=1.8283671438694\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.37018021941185\n",
      "Finished epoch, cumulative loss = 10.02852949500084\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=1.8019557297229767\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.42335259914398193\n",
      "Finished epoch, cumulative loss = 9.857509166002274\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=1.7670911252498627\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.36238929629325867\n",
      "Finished epoch, cumulative loss = 9.764531403779984\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=1.7495178580284119\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.3817223310470581\n",
      "Finished epoch, cumulative loss = 9.677230089902878\n",
      "Testset: Correct: 144.0/144. Cumulative Loss=1.7352652847766876\n",
      "Training classifier 5\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.6918436288833618\n",
      "Processed 992 examples, cumulative batch loss=18.973896205425262\n",
      "Finished epoch, cumulative loss = 27.56658634543419\n",
      "Testset: Correct: 247.0/261. Cumulative Loss=4.523539364337921\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.5195484161376953\n",
      "Processed 992 examples, cumulative batch loss=15.335238963365555\n",
      "Finished epoch, cumulative loss = 22.731065958738327\n",
      "Testset: Correct: 248.0/261. Cumulative Loss=4.070071637630463\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.4541192650794983\n",
      "Processed 992 examples, cumulative batch loss=14.055761486291885\n",
      "Finished epoch, cumulative loss = 21.332090586423874\n",
      "Testset: Correct: 248.0/261. Cumulative Loss=3.8352215588092804\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.4705904424190521\n",
      "Processed 992 examples, cumulative batch loss=13.672972321510315\n",
      "Finished epoch, cumulative loss = 20.486662089824677\n",
      "Testset: Correct: 249.0/261. Cumulative Loss=3.7191627621650696\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.40586692094802856\n",
      "Processed 992 examples, cumulative batch loss=13.228797405958176\n",
      "Finished epoch, cumulative loss = 19.860151827335358\n",
      "Testset: Correct: 249.0/261. Cumulative Loss=3.664620101451874\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.42014357447624207\n",
      "Processed 992 examples, cumulative batch loss=12.93531659245491\n",
      "Finished epoch, cumulative loss = 19.532261699438095\n",
      "Testset: Correct: 249.0/261. Cumulative Loss=3.6094409227371216\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.4576209783554077\n",
      "Processed 992 examples, cumulative batch loss=12.572155743837357\n",
      "Finished epoch, cumulative loss = 19.642044574022293\n",
      "Testset: Correct: 250.0/261. Cumulative Loss=3.502401649951935\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.3882758915424347\n",
      "Processed 992 examples, cumulative batch loss=12.500584453344345\n",
      "Finished epoch, cumulative loss = 19.12228825688362\n",
      "Testset: Correct: 250.0/261. Cumulative Loss=3.4775176644325256\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.38852429389953613\n",
      "Processed 992 examples, cumulative batch loss=12.467696964740753\n",
      "Finished epoch, cumulative loss = 18.816418886184692\n",
      "Testset: Correct: 250.0/261. Cumulative Loss=3.4492030441761017\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.3961830735206604\n",
      "Processed 992 examples, cumulative batch loss=12.312035381793976\n",
      "Finished epoch, cumulative loss = 18.635818034410477\n",
      "Testset: Correct: 250.0/261. Cumulative Loss=3.582908809185028\n",
      "Training classifier 6\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.6889113783836365\n",
      "Processed 992 examples, cumulative batch loss=16.73012888431549\n",
      "Processed 1952 examples, cumulative batch loss=29.155124366283417\n",
      "Finished epoch, cumulative loss = 35.95476275682449\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=5.235504537820816\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.3747067153453827\n",
      "Processed 992 examples, cumulative batch loss=11.541387647390366\n",
      "Processed 1952 examples, cumulative batch loss=22.33510521054268\n",
      "Finished epoch, cumulative loss = 28.827189028263092\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.930136680603027\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.3339497745037079\n",
      "Processed 992 examples, cumulative batch loss=11.034619331359863\n",
      "Processed 1952 examples, cumulative batch loss=21.52321746945381\n",
      "Finished epoch, cumulative loss = 27.76942503452301\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.819502830505371\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.3253461718559265\n",
      "Processed 992 examples, cumulative batch loss=10.713027775287628\n",
      "Processed 1952 examples, cumulative batch loss=21.03211808204651\n",
      "Finished epoch, cumulative loss = 27.31066757440567\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.764068603515625\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.3556985557079315\n",
      "Processed 992 examples, cumulative batch loss=10.72237515449524\n",
      "Processed 1952 examples, cumulative batch loss=20.897484362125397\n",
      "Finished epoch, cumulative loss = 27.041953921318054\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.729433566331863\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.36305999755859375\n",
      "Processed 992 examples, cumulative batch loss=10.597110509872437\n",
      "Processed 1952 examples, cumulative batch loss=20.778199702501297\n",
      "Finished epoch, cumulative loss = 26.87356024980545\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.706907391548157\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.3204953372478485\n",
      "Processed 992 examples, cumulative batch loss=10.607454270124435\n",
      "Processed 1952 examples, cumulative batch loss=20.69304183125496\n",
      "Finished epoch, cumulative loss = 26.751053273677826\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.689315438270569\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.33379030227661133\n",
      "Processed 992 examples, cumulative batch loss=10.473119050264359\n",
      "Processed 1952 examples, cumulative batch loss=20.60254779458046\n",
      "Finished epoch, cumulative loss = 26.654375284910202\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.675861418247223\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.33897024393081665\n",
      "Processed 992 examples, cumulative batch loss=10.356640666723251\n",
      "Processed 1952 examples, cumulative batch loss=20.486191391944885\n",
      "Finished epoch, cumulative loss = 26.61792379617691\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.670748859643936\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.3282027542591095\n",
      "Processed 992 examples, cumulative batch loss=10.32954517006874\n",
      "Processed 1952 examples, cumulative batch loss=20.51602879166603\n",
      "Finished epoch, cumulative loss = 26.533840656280518\n",
      "Testset: Correct: 438.0/444. Cumulative Loss=4.660191506147385\n",
      "Training classifier 7\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.6959604620933533\n",
      "Processed 992 examples, cumulative batch loss=17.03570306301117\n",
      "Finished epoch, cumulative loss = 29.354581892490387\n",
      "Testset: Correct: 310.0/330. Cumulative Loss=4.548465460538864\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.4137458801269531\n",
      "Processed 992 examples, cumulative batch loss=12.609465956687927\n",
      "Finished epoch, cumulative loss = 23.784696727991104\n",
      "Testset: Correct: 310.0/330. Cumulative Loss=4.26500540971756\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.3783182203769684\n",
      "Processed 992 examples, cumulative batch loss=12.046646058559418\n",
      "Finished epoch, cumulative loss = 22.74734655022621\n",
      "Testset: Correct: 311.0/330. Cumulative Loss=4.134731858968735\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.3608630299568176\n",
      "Processed 992 examples, cumulative batch loss=11.782701879739761\n",
      "Finished epoch, cumulative loss = 22.322569727897644\n",
      "Testset: Correct: 312.0/330. Cumulative Loss=4.107268005609512\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.4172590374946594\n",
      "Processed 992 examples, cumulative batch loss=11.587622851133347\n",
      "Finished epoch, cumulative loss = 22.061598241329193\n",
      "Testset: Correct: 313.0/330. Cumulative Loss=4.0635345578193665\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.45879802107810974\n",
      "Processed 992 examples, cumulative batch loss=11.55732998251915\n",
      "Finished epoch, cumulative loss = 21.8962984085083\n",
      "Testset: Correct: 313.0/330. Cumulative Loss=4.037601411342621\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.3567958474159241\n",
      "Processed 992 examples, cumulative batch loss=11.239168524742126\n",
      "Finished epoch, cumulative loss = 21.830340296030045\n",
      "Testset: Correct: 313.0/330. Cumulative Loss=4.025398463010788\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.347493976354599\n",
      "Processed 992 examples, cumulative batch loss=11.521223843097687\n",
      "Finished epoch, cumulative loss = 21.795980513095856\n",
      "Testset: Correct: 313.0/330. Cumulative Loss=4.051546484231949\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.41927996277809143\n",
      "Processed 992 examples, cumulative batch loss=11.388077765703201\n",
      "Finished epoch, cumulative loss = 21.636266440153122\n",
      "Testset: Correct: 313.0/330. Cumulative Loss=4.059795558452606\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.33013495802879333\n",
      "Processed 992 examples, cumulative batch loss=11.308861166238785\n",
      "Finished epoch, cumulative loss = 21.55343922972679\n",
      "Testset: Correct: 313.0/330. Cumulative Loss=3.983159899711609\n",
      "Training classifier 8\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.6876848936080933\n",
      "Processed 992 examples, cumulative batch loss=18.43336933851242\n",
      "Finished epoch, cumulative loss = 22.895119577646255\n",
      "Testset: Correct: 207.0/224. Cumulative Loss=3.5285989940166473\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.48188260197639465\n",
      "Processed 992 examples, cumulative batch loss=14.359583675861359\n",
      "Finished epoch, cumulative loss = 18.351763248443604\n",
      "Testset: Correct: 209.0/224. Cumulative Loss=3.150590658187866\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.437863290309906\n",
      "Processed 992 examples, cumulative batch loss=13.149214506149292\n",
      "Finished epoch, cumulative loss = 16.937962532043457\n",
      "Testset: Correct: 209.0/224. Cumulative Loss=2.998885452747345\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.4277208745479584\n",
      "Processed 992 examples, cumulative batch loss=12.655061066150665\n",
      "Finished epoch, cumulative loss = 16.31422659754753\n",
      "Testset: Correct: 210.0/224. Cumulative Loss=2.9135527312755585\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.3679398000240326\n",
      "Processed 992 examples, cumulative batch loss=12.340578377246857\n",
      "Finished epoch, cumulative loss = 15.926310628652573\n",
      "Testset: Correct: 210.0/224. Cumulative Loss=2.8584481477737427\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.41593000292778015\n",
      "Processed 992 examples, cumulative batch loss=12.198119163513184\n",
      "Finished epoch, cumulative loss = 15.689176827669144\n",
      "Testset: Correct: 211.0/224. Cumulative Loss=2.818870574235916\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.38201409578323364\n",
      "Processed 992 examples, cumulative batch loss=12.016025185585022\n",
      "Finished epoch, cumulative loss = 15.478681296110153\n",
      "Testset: Correct: 211.0/224. Cumulative Loss=2.7891686856746674\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.3561601936817169\n",
      "Processed 992 examples, cumulative batch loss=11.931969404220581\n",
      "Finished epoch, cumulative loss = 15.368275552988052\n",
      "Testset: Correct: 212.0/224. Cumulative Loss=2.7656653225421906\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.40370866656303406\n",
      "Processed 992 examples, cumulative batch loss=11.846007376909256\n",
      "Finished epoch, cumulative loss = 15.22677966952324\n",
      "Testset: Correct: 212.0/224. Cumulative Loss=2.7466814815998077\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.3943329155445099\n",
      "Processed 992 examples, cumulative batch loss=11.68027526140213\n",
      "Finished epoch, cumulative loss = 15.162716716527939\n",
      "Testset: Correct: 212.0/224. Cumulative Loss=2.7298881709575653\n",
      "Training classifier 9\n",
      "Epoch 0\n",
      "Processed 32 examples, cumulative batch loss=0.677211582660675\n",
      "Finished epoch, cumulative loss = 8.793931871652603\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.331442654132843\n",
      "Epoch 1\n",
      "Processed 32 examples, cumulative batch loss=0.4677169919013977\n",
      "Finished epoch, cumulative loss = 6.629019230604172\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.1441232562065125\n",
      "Epoch 2\n",
      "Processed 32 examples, cumulative batch loss=0.38463467359542847\n",
      "Finished epoch, cumulative loss = 5.9983793795108795\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.0779064297676086\n",
      "Epoch 3\n",
      "Processed 32 examples, cumulative batch loss=0.3585231602191925\n",
      "Finished epoch, cumulative loss = 5.76102602481842\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.0566959083080292\n",
      "Epoch 4\n",
      "Processed 32 examples, cumulative batch loss=0.3726651966571808\n",
      "Finished epoch, cumulative loss = 5.645317167043686\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.0359641909599304\n",
      "Epoch 5\n",
      "Processed 32 examples, cumulative batch loss=0.3623417615890503\n",
      "Finished epoch, cumulative loss = 5.57198703289032\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.0233198404312134\n",
      "Epoch 6\n",
      "Processed 32 examples, cumulative batch loss=0.33703598380088806\n",
      "Finished epoch, cumulative loss = 5.521943598985672\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.0194905698299408\n",
      "Epoch 7\n",
      "Processed 32 examples, cumulative batch loss=0.36187297105789185\n",
      "Finished epoch, cumulative loss = 5.483723402023315\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.0095283389091492\n",
      "Epoch 8\n",
      "Processed 32 examples, cumulative batch loss=0.33966711163520813\n",
      "Finished epoch, cumulative loss = 5.454594850540161\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.0114502608776093\n",
      "Epoch 9\n",
      "Processed 32 examples, cumulative batch loss=0.3450704514980316\n",
      "Finished epoch, cumulative loss = 5.4299066960811615\n",
      "Testset: Correct: 88.0/90. Cumulative Loss=1.0003580152988434\n"
     ]
    }
   ],
   "source": [
    "nbmodels2 = [BinaryClassifier() for i in range(10)]\n",
    "# Actual Training\n",
    "for i in range(10):\n",
    "    print('Training classifier', i)\n",
    "    epochs = 10 # all you need\n",
    "    lr = 0.01 #can probs do 0.1\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    nbmodels2[i].to(device)\n",
    "    optim = torch.optim.AdamW(nbmodels2[i].parameters(), lr = lr, weight_decay = 0.01)\n",
    "    WEtrainLoader = DataLoader(WeightedEmbedDataset(0, glove50, i, 1), batch_size=32, shuffle=True)\n",
    "    WEtestLoader = DataLoader(WeightedEmbedDataset(1, glove50, i, 1), batch_size=32, shuffle=True)\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        print(f\"Epoch {j}\")\n",
    "        trainloop(WEtrainLoader, nbmodels2[i], loss, optim)\n",
    "        testloop(WEtestLoader, nbmodels2[i], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "64d32da7-702b-4001-b7e7-0c82ef469d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store weights\n",
    "\n",
    "for i in range(10):\n",
    "    torch.save(nbmodels2[i], 'linear_model'+str(i)+\"weights.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb236a-3883-48ef-8a0d-63c4540cd41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nb_average(models, embedding, txt):\n",
    "    txt = txt.split() \n",
    "    res = np.zeros(10)\n",
    "    num_words = 0\n",
    "    for i in range(10):\n",
    "        embed = np.zeros(embedding.SIZE)\n",
    "        for word in txt:\n",
    "            this_embedding = embedding.get_word_embedding(word.lower())\n",
    "            if this_embedding.max() != 0 or this_embedding.min() != 0:\n",
    "                embed += weightScheme.word_to_weight(word.lower(), i)*this_embedding\n",
    "                num_words += 1\n",
    "        embed /= num_words\n",
    "        tmp = models[i](torch.tensor(embed).unsqueeze(0).float())[0]\n",
    "        print(tmp)\n",
    "        res[i] = models[i](torch.tensor(embed).unsqueeze(0).float())[0][1].item()\n",
    "\n",
    "    #print(res)\n",
    "    return res\n",
    "\n",
    "eval_embeddings(nbmodels2, glove50, test_nb_average, outfile=\"benchmark_nb_averaged_embedding_regression.txt\", size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "494da2cd-53af-498e-8859-deb64fb35464",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelSimple, 'naiveEmbeddingregression.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "34276c54-8415-49c7-b699-2be0ea91d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0056, 0.9937]], grad_fn=<SigmoidBackward>) 1\n"
     ]
    }
   ],
   "source": [
    "a,b = WeightedEmbedDataset(1, glove50, i, 1).__getitem__(12)\n",
    "print(nbmodels[1](a.unsqueeze(0)),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "28dc3809-2c37-42e8-84ec-2d46fdfbd725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1193/1193 [00:32<00:00, 36.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Finalise Data\n",
    "# In the interests of time SVMs will not be tested on embeddings. Based on the same amount of training data\n",
    "# Linear regression performed worse than SVMs thus we conclude that embeddings are not as good \n",
    "# It was hoped that the massive corpus used to train embeddings would counteract its limited\n",
    "# Dimensionality. This proved to be not the case\n",
    "WikiTestset = WikiDataset(2)\n",
    "Xall = Xtrain[:]\n",
    "yall = ytrain[:]\n",
    "Xall.extend(Xval)\n",
    "yall.extend(yval)\n",
    "for i in tqdm(range(WikiTestset.__len__())):\n",
    "    #if i % 50 == 0:\n",
    "    #    print(f\"Loaded {i} Test Examples\")\n",
    "    a,b = ValSet.__getitem__(i)\n",
    "    Xall.append(a)\n",
    "    yall.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "1780c36e-3e4c-425e-b3cc-d34480b43225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset xtrain\n",
    "Xtrain = Xtrain[:5567]\n",
    "ytrain = ytrain[:5567]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c85bd5a9-e721-4285-b470-07d69ce7d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training!\n",
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                              | 84/3342 [00:04<01:50, 29.58it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      " 95%|██████████████████████████████████████████████████████████████████████████    | 3175/3342 [01:20<00:03, 50.56it/s]C:\\Users\\junhu\\Desktop\\Machine Learning\\Study concentration\\Model Construction\\benchmark_base.py:60: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return bag/bag.sum() # normalise\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3342/3342 [01:23<00:00, 39.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902/1820 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SVM_tester4(CategoriserBase):\n",
    "    def __init__(self):\n",
    "        super(SVM_tester4, self).__init__()\n",
    "    \n",
    "    def precomp(self):\n",
    "        # override\n",
    "        print('training!')\n",
    "        self.model = svm.LinearSVC(C=30, max_iter=2000)\n",
    "        self.model.fit(Xall, yall)\n",
    "        print('Finished training')\n",
    "    def predict(self, bag):\n",
    "        return self.model.decision_function([bag])[0]\n",
    "    \n",
    "SVM_instance4 = SVM_tester4()\n",
    "SVM_instance4.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "0aa41881-f986-4b75-aca5-0e4bed7b46ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVMimproved.joblib']"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(SVM_instance4.model, 'SVMimproved.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "dff886b1-c8c9-4899-be2e-a8be18965b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1997361311767847 1.847604202035786 1.847604202035786\n",
      "0.04260568520389185 1.388670694584281 1.388670694584281\n",
      "-0.4690109366774884 2.3864654453680583 2.3864654453680583\n",
      "-0.9113165433487858 2.6282615649993293 2.6282615649993293\n",
      "-0.22982753297656444 2.825299457995655 2.825299457995655\n",
      "1.6869260211468369 1.9901001183093374 1.9901001183093374\n",
      "-0.8357298906760633 1.7637183008441486 1.7637183008441486\n",
      "-0.6072786313466043 2.1663809458946712 2.1663809458946712\n",
      "-0.4496622068992302 2.412361195992677 2.412361195992677\n",
      "-0.4921296856695889 3.5984663630475193 3.5984663630475193\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "     print(weightScheme.word_to_weight('wikipedia', i),weightScheme.word_to_weight('a', i),weightScheme.word_to_weight('an', i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057fa43-b936-4f79-9dd8-f78d5c878d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nb_filtering(models, embedding, txt):\n",
    "    txt = txt.split() \n",
    "    res = np.zeros(10)\n",
    "    num_words = 0\n",
    "    for i in range(10):\n",
    "        embed = np.zeros(embedding.SIZE)\n",
    "        for word in txt:\n",
    "            if word in STOP_WORDS:\n",
    "                continue\n",
    "            this_embedding = embedding.get_word_embedding(word.lower())\n",
    "            \n",
    "            if this_embedding.max() != 0 or this_embedding.min() != 0:\n",
    "                embed += weightScheme.word_to_weight(word.lower(), i)*this_embedding\n",
    "                num_words += 1\n",
    "        embed /= num_words\n",
    "        tmp = models[i](torch.tensor(embed).unsqueeze(0).float())[0]\n",
    "        #print(tmp)\n",
    "        res[i] = models[i](torch.tensor(embed).unsqueeze(0).float())[0][1].item()\n",
    "\n",
    "    #print(res)\n",
    "    return res\n",
    "\n",
    "eval_embeddings(nbmodels2, glove50, test_nb_average, outfile=\"benchmark_nb_averaged_embedding_regression.txt\", size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "b282542d-67c1-4b43-a7f6-48e38394d511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3342/3342 [50:46<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378/1822 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_sentence_svm(model, embedding, txt):\n",
    "    txt = txt.split(' ') \n",
    "    res = torch.zeros(10)\n",
    "    WORDS_PER_SENTENCE = 40\n",
    "    pt = 0\n",
    "    while pt < len(txt):\n",
    "        sentence = ' '.join(txt[pt:pt+WORDS_PER_SENTENCE])\n",
    "        if len(sentence) >= 40: \n",
    "            #print(sentence)\n",
    "            bag = SVM_instance4.preprocess(sentence)\n",
    "            s = bag.sum()\n",
    "            if bag.sum() != 0:\n",
    "                tmp = SVM_instance4.predict(SVM_instance4.preprocess(sentence)/s)\n",
    "                if tmp.max() > -0.75:\n",
    "                    res[tmp.argmax()] += 1\n",
    "        pt += int(WORDS_PER_SENTENCE*0.75)\n",
    "    #print(res)\n",
    "    return res\n",
    "\n",
    "eval_embeddings(SVM_instance4.model, glove50, test_sentence_svm, outfile=\"benchmark_smv_sentence.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3bde8-c27b-4b87-b0e4-e86232e623aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
